"""
–ü—Ä–æ—Å—Ç–∞—è VLM –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ Mac M2
"""
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer, AutoProcessor
from src.utils.device import get_device

class SimpleVLM(nn.Module):
    """–£–ø—Ä–æ—â—ë–Ω–Ω–∞—è Vision-Language –º–æ–¥–µ–ª—å –¥–ª—è Mac"""

    def __init__(self, config):
        super().__init__()

        self.config = config
        self.device = get_device(verbose=False)

        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SimpleVLM...")

        # Vision encoder (CLIP)
        print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ vision encoder: {config['model']['vision_encoder']}")
        self.vision_model = AutoModel.from_pretrained(
            config['model']['vision_encoder'],
            torch_dtype=torch.float16 if config['device']['mixed_precision'] else torch.float32
        )

        # Language model (Russian GPT)
        print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ language model: {config['model']['language_model']}")
        self.language_model = AutoModel.from_pretrained(
            config['model']['language_model'],
            torch_dtype=torch.float16 if config['device']['mixed_precision'] else torch.float32
        )

        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        # –î–ª—è CLIP: hidden_size –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ vision_config
        vision_dim = self.vision_model.config.vision_config.hidden_size
        language_dim = self.language_model.config.hidden_size

        print(f"   Vision dimension: {vision_dim}")
        print(f"   Language dimension: {language_dim}")

        # –ü—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π
        self.projection = nn.Linear(vision_dim, language_dim)

        # –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å —á–∞—Å—Ç—å —Å–ª–æ—ë–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        self._freeze_layers()

        # –ü–µ—Ä–µ–Ω–µ—Å—Ç–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.to(self.device)

        print(f" –ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.device}")
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in self.parameters()):,}")

    def _freeze_layers(self):
        """–ó–∞–º–æ—Ä–æ–∑–∫–∞ —Å–ª–æ—ë–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏"""
        # –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å vision encoder (–∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ—ë–≤)
        for name, param in list(self.vision_model.named_parameters()):
            if 'vision_model.encoder.layers.11' not in name:  # –ù–µ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π
                param.requires_grad = False

        # –ó–∞–º–æ—Ä–æ–∑–∏—Ç—å language model (–∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ—ë–≤)
        for name, param in list(self.language_model.named_parameters()):
            if 'h.11' not in name:  # –ù–µ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π GPT
                param.requires_grad = False

        print("   –ó–∞–º–æ—Ä–æ–∂–µ–Ω—ã —Å–ª–æ–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏")
        print(f"   –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in self.parameters() if p.requires_grad):,}")

    def forward(self, images, input_ids, attention_mask):
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥"""
        # –í–∏–∑—É–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏
        vision_outputs = self.vision_model(pixel_values=images)

        # –î–ª—è CLIP –∏—Å–ø–æ–ª—å–∑—É–µ–º pooler_output –∏–ª–∏ last_hidden_state
        if hasattr(vision_outputs, 'pooler_output'):
            image_features = vision_outputs.pooler_output
        else:
            image_features = vision_outputs.last_hidden_state[:, 0, :]  # [CLS] token

        # –ü—Ä–æ–µ–∫—Ü–∏—è
        projected_features = self.projection(image_features)

        # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∏—á–∏
        text_outputs = self.language_model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π hidden state
        text_features = text_outputs.last_hidden_state

        # –ü—Ä–æ—Å—Ç–∞—è –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è (–≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ + —Ç–µ–∫—Å—Ç–æ–≤—ã–µ)
        # –†–∞—Å—à–∏—Ä—è–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ –¥–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö
        projected_features = projected_features.unsqueeze(1)  # [batch, 1, dim]

        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º
        combined_features = torch.cat([projected_features, text_features], dim=1)

        return combined_features

    def predict(self, image, question, tokenizer, processor, max_length=50):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞"""
        self.eval()

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_inputs = processor(images=image, return_tensors="pt")
        image_inputs = {k: v.to(self.device) for k, v in image_inputs.items()}

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞
        text_inputs = tokenizer(
            question,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        text_inputs = {k: v.to(self.device) for k, v in text_inputs.items()}

        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        with torch.no_grad():
            features = self.forward(
                images=image_inputs['pixel_values'],
                input_ids=text_inputs['input_ids'],
                attention_mask=text_inputs['attention_mask']
            )

        # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞: –≤–µ—Ä–Ω—É—Ç—å —Ä–∞–∑–º–µ—Ä —Ñ–∏—á–µ–π
        return f"–í–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç: [–º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞, —Ñ–∏—á–∏: {features.shape}]"